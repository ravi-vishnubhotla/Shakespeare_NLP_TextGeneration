{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ravi_NLP_Shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzM-frqrUrx9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95QAUIPQU22V"
      },
      "source": [
        "#open text | create vocab | char to index dict | int to character numpy | Final goal: encoded text using character to int dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naJZPqQMTm6S"
      },
      "source": [
        "text = open('shakespeare.txt',mode='r').read()\n",
        "vocab = sorted(set(text))\n",
        "char_to_int = {u:i for i, u in enumerate(vocab)}\n",
        "ind_to_char = np.array(vocab)\n",
        "encoded_text = np.array([char_to_int[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLQKNxBt6872",
        "outputId": "d4137ff0-37d2-4686-96aa-f5ecf27ecf2b"
      },
      "source": [
        "encoded_text[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKTHAEvD7Csy"
      },
      "source": [
        "#creating batches. \n",
        "#Figure out how many sequences are needed by analysing given text. Here for Shakespeare text if we take 120 characters we can get a good prediction from model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YBLKmSNSotG",
        "outputId": "dc18288d-aff1-40d0-fafe-73083aac09c6"
      },
      "source": [
        "seq_len = 120\n",
        "total_num_seq = len(text)//(seq_len+1)\n",
        "total_num_seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuXKgepzTJXm"
      },
      "source": [
        "# Create Training Sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWS0nY9YWDp8",
        "outputId": "9c81bf1e-ce22-4130-d69d-5fd31791994a"
      },
      "source": [
        "encoded_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5445609,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLb1PI-jUjmq",
        "outputId": "4bc65e19-3ceb-4975-f3ab-bcec1e6c363d"
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text) #input is encoded text array.\n",
        "print(type(char_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1EfchF8DXL0",
        "outputId": "b07a12a0-8e7a-4072-c3ae-e04ddabf86e5"
      },
      "source": [
        "print(char_dataset)\n",
        "len(char_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcgEJ72eDqzp",
        "outputId": "7a2b3836-3670-47a2-bf4d-070ee5dc0e0b"
      },
      "source": [
        "print(char_dataset.take(25))\n",
        "len(char_dataset.take(25))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TakeDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xODjPgqwWJmD",
        "outputId": "2749d657-fe58-48a7-bcc3-c8b05565d59c"
      },
      "source": [
        "for i in char_dataset.take(25):\n",
        "  print(i) #i is eager tensor\n",
        "  print(ind_to_char[i.numpy()]) # note that spaces are returned below. thats why we cant see anything\n",
        "type(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "\n",
            "\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(12, shape=(), dtype=int64)\n",
            "1\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "\n",
            "\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqMZoZU5Wh8D",
        "outputId": "7ff06073-7e54-44a4-dd0e-2900c39aa803"
      },
      "source": [
        "print(ind_to_char[12]) # output for index 12=1, above cell output shows this\n",
        "print(ind_to_char[0]) # space character thats why shows blank in below output\n",
        "print(ind_to_char[1]) # space character"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "\n",
            "\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqzRoyYnmSZN",
        "outputId": "025d4b61-5a4f-4746-9722-24e626eacbc7"
      },
      "source": [
        "seq_len #we defined this to be 120 by analysing shakespeare style"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOfyVkQGYCiL",
        "outputId": "aef395d4-4896-4316-cd34-c6e492ad04eb"
      },
      "source": [
        "sequences = char_dataset.batch(batch_size = seq_len+1, drop_remainder = True) #Combines consecutive elements of char_dataset into batches of size defined ('seq_len+1' in this case)\n",
        "sequences #outputs shape 121 because we gave seq_len=120(0 to 120 = 121). "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (121,), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCXza6wRmgaX",
        "outputId": "018e60e0-0989-4f99-8d4c-a81d5e7c1d7e"
      },
      "source": [
        "len(sequences) #length of each batch is 45005 characters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqNY92ivms6q",
        "outputId": "55bb0cf6-a82f-4cb1-d516-4f1a0db67a57"
      },
      "source": [
        "len(sequences) * (seq_len+1) #this should give total characters in text files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445605"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ImVBLqNm5Ag",
        "outputId": "d22e18e3-8f39-497f-b856-289a993d858c"
      },
      "source": [
        "len(text) #almost same as above cell, change in value is because we used // in division to round off decimal value for calculating total_num_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kOgWkvNEJ6T",
        "outputId": "01bc77e1-c95c-4708-bf01-b4f70565dc35"
      },
      "source": [
        "#print lengths and types of all above datasets adn show which method is used for conversion between lines\n",
        "print('Original text - || length:', len(text),'|| type:', type(text))\n",
        "print('\\t\\t###char_to_ind###')\n",
        "print('Encoded text - || length:', len(encoded_text),'|| type:', type(encoded_text), '|| shape:', encoded_text.shape)\n",
        "print('\\t\\t###tf.data.Dataset.from_tensor_slices(encoded_text)###')\n",
        "print('Char dataset - || length:', len(char_dataset),'|| type:', type(char_dataset),'\\n\\t\\t|| sample(5) length:', len(char_dataset.take(5)),'|| sample type:', type(char_dataset.take(5)))\n",
        "print('\\t\\t###char_dataset.batch(batch_size = seq_len+1, drop_remainder = True)###')\n",
        "print('sequences - || length:', len(sequences),'|| type:', type(sequences),'\\n\\t\\t|| sample(5) length:', len(sequences.take(5)),'|| sample type:', type(sequences.take(5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text - || length: 5445609 || type: <class 'str'>\n",
            "\t\t###char_to_ind###\n",
            "Encoded text - || length: 5445609 || type: <class 'numpy.ndarray'> || shape: (5445609,)\n",
            "\t\t###tf.data.Dataset.from_tensor_slices(encoded_text)###\n",
            "Char dataset - || length: 5445609 || type: <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'> \n",
            "\t\t|| sample(5) length: 5 || sample type: <class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>\n",
            "\t\t###char_dataset.batch(batch_size = seq_len+1, drop_remainder = True)###\n",
            "sequences - || length: 45005 || type: <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> \n",
            "\t\t|| sample(5) length: 5 || sample type: <class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFi87NlPm66p",
        "outputId": "14353f63-fc72-4cef-8ac8-3ff18152ce23"
      },
      "source": [
        "#examples of slicing array\n",
        "a = np.arange(1,5)\n",
        "print('Original Array a =',a,' Shape=',a.shape)\n",
        "print('sliced with a[:-1] =',a[:-1],' Shape=', a[:-1].shape)\n",
        "print('sliced with a[1:] =',a[1:],' Shape=',a[1:].shape)\n",
        "print('sliced with a[0:-1] =',a[0:-1],' Shape=',a[0:-1].shape)\n",
        "print('sliced with a[1:-1] =',a[1:-1],' Shape=',a[1:-1].shape)\n",
        "print('sliced with a[1:0] =',a[1:0],' Shape=',a[1:0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Array a = [1 2 3 4]  Shape= (4,)\n",
            "sliced with a[:-1] = [1 2 3]  Shape= (3,)\n",
            "sliced with a[1:] = [2 3 4]  Shape= (3,)\n",
            "sliced with a[0:-1] = [1 2 3]  Shape= (3,)\n",
            "sliced with a[1:-1] = [2 3]  Shape= (2,)\n",
            "sliced with a[1:0] = []  Shape= (0,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJamF9joor4f"
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "  input_txt = seq[:-1]\n",
        "  target_txt = seq[1:]\n",
        "  print('length of input text=', len(input_txt))\n",
        "  print('length of target text=', len(target_txt))\n",
        "  return input_txt, target_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgaBTh1qrI2U",
        "outputId": "4d27b34e-e33f-4d44-f348-6005994eb562"
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)\n",
        "print(type(dataset))\n",
        "print(len(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of input text= 120\n",
            "length of target text= 120\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
            "45005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWeT_PtDsTsT",
        "outputId": "5cdfb736-92c4-4522-80a9-ef8cbeedd46b"
      },
      "source": [
        "print(type(sequences))\n",
        "print(len(sequences))\n",
        "print('number of sequences=', seq_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
            "45005\n",
            "number of sequences= 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGDepxOrsXn1",
        "outputId": "1cf5a474-d272-4751-b339-caadbcd4e83e"
      },
      "source": [
        "for input_txt, target_txt in  dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print('--------------------------------------------------------')\n",
        "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
        "    print('--------------------------------------------------------')\n",
        "    print('\\n')\n",
        "    print(target_txt.numpy())\n",
        "    print('--------------------------------------------------------')\n",
        "    # There is an extra whitespace!\n",
        "    print(''.join(ind_to_char[target_txt.numpy()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
            "--------------------------------------------------------\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But\n",
            "--------------------------------------------------------\n",
            "\n",
            "\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
            "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
            " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
            " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
            "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
            "--------------------------------------------------------\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI0MdwIkvBDj"
      },
      "source": [
        "#Generate training batches by shuffling dataset and giving buffer size so that entire sequnce in memory is not shuffled\n",
        "batch_size = 128\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size,drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_cgrkLTv3US",
        "outputId": "b57b1368-81c4-4f1b-c17e-44bdd1928c2c"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQZ0p8eWvIub"
      },
      "source": [
        "#define variables for model\n",
        "vocab_size = len(vocab)\n",
        "input_length = seq_len #120\n",
        "embed_dim = 64\n",
        "rnn_neurons = 1026"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G05vJICWvJaL"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, GRU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r1Hc5Caya0g"
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9In2cizOycTs"
      },
      "source": [
        "#define our own loss function-to set from_logits to Trueas it cannot be set separately when defining model loss\n",
        "def sparse_cat_loss(y_true, y_pred):\n",
        "  sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBE7j7HkqgM0"
      },
      "source": [
        "def create_model(vocab_size,embed_dim,rnn_neurons,batch_size):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,output_dim=embed_dim,batch_input_shape=[batch_size, None]))\n",
        "  model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "  model.add(Dense(vocab_size))\n",
        "  model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIURwDYf8ucs"
      },
      "source": [
        "###stateful: Boolean (default False). If True, the last state\n",
        "###for each sample at index i in a batch will be used as initial\n",
        "###state for the sample of index i in the following batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I20Rodc0JyH"
      },
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embed_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK03_mrJz8Mo",
        "outputId": "ff0bbcb3-8c12-4fb1-ae74-f28d9c0adc79"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 64)           5376      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (128, None, 1026)         3361176   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 84)           86268     \n",
            "=================================================================\n",
            "Total params: 3,452,820\n",
            "Trainable params: 3,452,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68RTZfj0GJP"
      },
      "source": [
        "#train model. First train single batch before spending too much time on whole data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXei8t3hBy6m",
        "outputId": "1cce66ef-9d58-4624-ea91-fee8cc3715b3"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6UoPfBBCmZt",
        "outputId": "68be922b-04d1-4930-c194-35dd32f991b5"
      },
      "source": [
        "#printing above datasets for better understanding\n",
        "print('Original dataset:',dataset)\n",
        "print('length of Original dataset:',len(dataset))\n",
        "print('dataset.take(1):',dataset.take(1))\n",
        "print('dataset.take(1) length:',len(dataset.take(1)))\n",
        "print('input_example_batch shape, type:', input_example_batch.shape,type(input_example_batch), 'length=',len(input_example_batch))\n",
        "print('target_example_batch shape, type:', target_example_batch.shape, type(target_example_batch), 'length=',len(target_example_batch))\n",
        "print('example_batch_predictions shape, type:', example_batch_predictions.shape, type(example_batch_predictions), 'length=', len(example_batch_predictions))\n",
        "print('batch_size =', batch_size)\n",
        "print('seq_len =', seq_len)\n",
        "print('vocab_size =', vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original dataset: <BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>\n",
            "length of Original dataset: 351\n",
            "dataset.take(1): <TakeDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>\n",
            "dataset.take(1) length: 1\n",
            "input_example_batch shape, type: (128, 120) <class 'tensorflow.python.framework.ops.EagerTensor'> length= 128\n",
            "target_example_batch shape, type: (128, 120) <class 'tensorflow.python.framework.ops.EagerTensor'> length= 128\n",
            "example_batch_predictions shape, type: (128, 120, 84) <class 'tensorflow.python.framework.ops.EagerTensor'> length= 128\n",
            "batch_size = 128\n",
            "seq_len = 120\n",
            "vocab_size = 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri9lgvPNAaJe",
        "outputId": "e7197cf3-4ba5-4148-c81b-4445d3beef98"
      },
      "source": [
        "example_batch_predictions[0] #all these outputs are log of probabilities of a character in that index level"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(120, 84), dtype=float32, numpy=\n",
              "array([[ 0.00515868, -0.00341785,  0.00498576, ..., -0.00159612,\n",
              "         0.00137965, -0.00054629],\n",
              "       [ 0.00858305,  0.00104567,  0.00378551, ..., -0.00351187,\n",
              "         0.00077107, -0.00015057],\n",
              "       [-0.002416  ,  0.0080875 , -0.0006921 , ..., -0.00193997,\n",
              "        -0.00410851, -0.0010994 ],\n",
              "       ...,\n",
              "       [-0.00193862,  0.0005345 ,  0.00390209, ..., -0.00255957,\n",
              "        -0.00244223, -0.00041294],\n",
              "       [-0.00681666, -0.00385651,  0.0045778 , ...,  0.00682031,\n",
              "         0.00919648, -0.00182251],\n",
              "       [ 0.00438812, -0.00174779,  0.00219739, ...,  0.00734712,\n",
              "         0.00898124,  0.0020445 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91X-Cvc9JxMt"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZR1g0kYB4nX",
        "outputId": "ddddb0d1-b834-41e1-9fec-56076665320a"
      },
      "source": [
        "print(type(sampled_indices))\n",
        "print(sampled_indices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(120, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaul5q-mBVkw"
      },
      "source": [
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy() # .numpy is converting eagertensor to numpy and axis = -1 is reshaping it to (120,). (120,1) array will be displayed in a lonmg vertical way. 120,0 is convenient horizontal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxRGL-WKB_UX",
        "outputId": "067b73de-893a-457a-eecd-43e0d53640ab"
      },
      "source": [
        "print(type(sampled_indices))\n",
        "print(sampled_indices.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(120,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAj3WHvRAa-P",
        "outputId": "355fd30a-bc70-4bba-f7c3-0326cd0cc9e6"
      },
      "source": [
        "ind_to_char[sampled_indices]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['R', 'D', '\\n', 'H', 'j', '[', 'O', '?', 'l', 'R', 'u', 'N', '5',\n",
              "       '?', ':', 'w', '4', 'l', '|', '\"', '6', '9', 'S', ';', 'A', 'x',\n",
              "       ';', '>', '|', '>', 'N', 'v', '!', '`', 'f', 'H', 'y', '8', 'p',\n",
              "       'l', 'x', '0', 'O', 's', ':', 'v', 'Z', 'o', 't', 'Z', 'T', 'h',\n",
              "       'b', ';', ')', 'l', 'z', 'r', 'C', '\\n', '4', 'M', 'W', '1', 'J',\n",
              "       't', '2', 'F', '|', '<', 'p', '|', '[', ',', 'r', 'D', 'I', '>',\n",
              "       'a', '0', 'n', 'e', 'i', 'y', '-', 'G', 'a', '1', 'u', '0', 'V',\n",
              "       '.', ';', 'w', '9', 'f', 'm', 'T', 'z', 'j', 'g', '8', 'u', 'j',\n",
              "       'y', ',', 'd', ']', ')', 'n', 'b', 'z', '6', ';', '`', '-', 'p',\n",
              "       'S', 'C', 'u'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrY-Ur26Fbcu",
        "outputId": "2fac18c9-ebdb-4b2f-ec5b-2ceaeb7397ee"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5APKUwYqB3qP"
      },
      "source": [
        "#train\n",
        "#epochs = 30\n",
        "#model.fit(dataset, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht_kx8EXBT_E"
      },
      "source": [
        "model.save('ravi_shakespeare_gen.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqNpL_xDBQGz"
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT0BJ8ltF-28"
      },
      "source": [
        "model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1)#changing batch size here to 1. It was 128 before"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcjCRWTyGLyl"
      },
      "source": [
        "model.load_weights('shakespeare_gen.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsQIsdXtGSUk"
      },
      "source": [
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfRsXdIOGgWk",
        "outputId": "3ab205e6-0a63-4462-a494-dad87e47194d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 64)             5376      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1026)           3361176   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 84)             86268     \n",
            "=================================================================\n",
            "Total params: 3,452,820\n",
            "Trainable params: 3,452,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx30PKEyGiB0"
      },
      "source": [
        "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
        "\n",
        "  num_generate = gen_size\n",
        "  input_eval = [char_to_int[s] for s in start_seed]\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "\n",
        "  text_generated = []\n",
        "  temperature = temp\n",
        "\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions,0) #to remove batch shape dimension\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy() #to get single ID\n",
        "    input_eval = tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed+\"\".join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk2dXINoNQoM"
      },
      "source": [
        "print(generate_text(model,\"Romeo and Juliet\", gen_size=2000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QdyT0soNafO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}